# Awesome-LLM4Kernel

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![Awesome LLM4Kernel](https://img.shields.io/badge/Awesome-LLM4Kernel-blue)](https://github.com/topics/awesome)

Awesome-LLM4Kernel: A curated list of papers with codes related to LLM-based kernel generation and optimization.

## ğŸ“š Table of Contents

- [ğŸ“ By Generative Paradigm](#-by-generative-paradigm)
    - [âš¡ Torch Related](#-torch-related)
    - [ğŸ’» C / C++ Related](#-c--c-related)
    - [ğŸ”¤ Natural Language Related](#-natural-language-related)
    - [ğŸ§© Other Paradigms](#-other-paradigms)
- [ğŸ¯ By Technical Focus](#-by-technical-focus)
    - [ğŸ“Š Benchmarks](#-benchmarks)
    - [âš™ï¸ Fine-tuning for Kernel Generation](#ï¸-fine-tuning-for-kernel-generation)
    - [ğŸ” Self-Refinement & Iterative Optimization](#-self-refinement--iterative-optimization)
    - [ğŸ§© Other Methods](#-other-methods)
- [ğŸ“˜ All Papers (Sorted By Date)](#-all-papers-sorted-by-date)
- [âœ¨ Contributing](#-contributing)

---

## ğŸ“ By Generative Paradigm

### âš¡ Torch Related

- [KernelBench](#202502-icml-2025-kernelbench-can-llms-write-efficient-gpu-kernels): Torch2CUDA, (Torch2Triton, Torch2CuTe, Torch2TileLang)
- [TritonBench](#202502-tritonbench-benchmarking-large-language-model-capabilities-for-generating-triton-operators): Torch2Triton
- [KernelLLM](#202504-making-kernel-development-more-accessible-with-kernelllm): Torch2Triton
- [GPU Kernel Scientist](#202506-gpu-kernel-scientist-an-llm-driven-framework-for-iterative-kernel-optimization): Torch2HIP (AMD)
- [AutoTriton](#202507-autotriton-automatic-triton-programming-with-reinforcement-learning-in-llms): Torch2Triton
- [Kevin](#202507-kevin-multi-turn-rl-for-generating-cuda-kernels): Torch2CUDA
- [CUDA-L1](#202507-cuda-l1-improving-cuda-optimization-via-contrastive-reinforcement-learning): Torch2CUDA
- [MultiKernelBench](#202507-multikernelbench-a-multi-platform-benchmark-for-kernel-generation): Torch2CUDA, Torch2AscendC (Huawei NPU), Torch2Pallas (Google TPU)
- [GEAK](#202507-geak-introducing-triton-kernel-ai-agent--evaluation-benchmarks): Torch2Triton
- [CudaLLM](#202508-cudallm-training-language-models-to-generate-high-performance-cuda-kernels): Torch2CUDA
- [Robust-KBench](#202509-towards-robust-agentic-cuda-kernel-benchmarking-verification-and-optimization): Torch2CUDA
- [ConCuR](#202510-concur-conciseness-makes-state-of-the-art-kernel-generation): Torch2CUDA
- [TritonRL](#202510-tritonrl-training-llms-to-think-and-code-triton-without-cheating): Torch2Triton
- [STARK](#202510-stark-strategic-team-of-agents-for-refining-kernels): Torch2CUDA
- [TritonGym](#202510-tritongym-a-benchmark-for-agentic-llm-workflows-in-triton-gpu-code-generation): Torch2Triton
- [CudaForge](#202511-cudaforge-an-agent-framework-with-hardware-feedback-for-cuda-kernel-optimization): Torch2CUDA
- [PRAGMA](#202511-pragma-a-profiling-reasoned-multi-agent-framework-for-automatic-kernel-optimization): Torch2CUDA
- [KernelFalcon](#202511-kernelfalcon-autonomous-gpu-kernel-generation-via-deep-agents): Torch2CUDA
- [PIKE-B](#202511-optimizing-pytorch-inference-with-llm-based-multi-agent-systems): Torch2CUDA
- [QiMeng-Kernel](#202511-aaai-2026-qimeng-kernel-macro-thinking-micro-coding-paradigm-for-llm-based-high-performance-gpu-kernel-generation): Torch2CUDA
- [TritonForge](#202512-tritonforge-profiling-guided-framework-for-automated-triton-kernel-optimization): Torch2Triton

### ğŸ’» C / C++ Related

- [HPCTransCompile](#202506-hpctranscompile-an-ai-compiler-generated-dataset-for-high-performance-cuda-transpilation-and-llm-preliminary-exploration): CUDA2C / C++
- [QiMeng-MuPa](#202506-neurips-2025-qimeng-mupa-mutual-supervised-learning-for-sequential-to-parallel-code-translation): C++2CUDA

### ğŸ”¤ Natural Language Related

- [ComputeEval](#202504-computeeval-evaluating-large-language-models-for-cuda-code-generation): Text2CUDA
- [CUDA-LLM](#202506-cuda-llm-llms-can-write-efficient-cuda-kernels): Text2CUDA

### ğŸ§© Other Paradigms

- [SwizzlePerf](#202508-swizzleperf-hardware-aware-llms-for-gpu-kernel-performance-optimization): Specific Kernels (`GEMM`, `fused element-wise`, `LayerNorm`, `Softmax`, `naive sparse matrix vector multiplication (SpMV)`, `transpose`, `Black-Scholes`, `finite-difference time-domain (FDTD) 2D`, `Smith-Waterman`, `Stencil 2D`)
- [Astra](#202509-astra-a-multi-agent-system-for-gpu-kernel-performance-optimization): CUDA2CUDA, Specific Kernels (`silu_and_mul`, `fused_add_rmsnorm`, `merge_attn_states_lse`)
- [RISC-V Kernels](#202509-evolution-of-kernels-automated-risc-v-kernel-optimization-with-large-language-models): RISC-V Kernels Optimization
- [EvoEngineer](#202510-evoengineer-mastering-automated-cuda-kernel-code-evolution-with-large-language-models): CUDA2CUDA
- [Performance Tool](#202510-integrating-performance-tools-in-model-reasoning-for-gpu-kernel-optimization): CUDA2CUDA
- [ReGraphT](#202510-from-large-to-small-transferring-cuda-optimization-expertise-via-reasoning-graph): CUDA2CUDA
- [SparseRL](#202510-mastering-sparse-cuda-generation-through-pretrained-models-and-deep-reinforcement-learning): CUDA
- [MaxCode](#202510-maxcode-a-max-reward-reinforcement-learning-framework-for-automated-code-optimization): CUDA2CUDA
- [CUDA-L2](#202512-cuda-l2-surpassing-cublas-performance-for-matrix-multiplication-through-reinforcement-learning): `Half-precision General Matrix Multiply (HGEMM)`

## ğŸ¯ By Technical Focus

### ğŸ“Š Benchmarks

- [KernelBench](#202502-icml-2025-kernelbench-can-llms-write-efficient-gpu-kernels): Torch2CUDA, (Torch2Triton, Torch2CuTe, Torch2TileLang)
- [TritonBench](#202502-tritonbench-benchmarking-large-language-model-capabilities-for-generating-triton-operators): Torch2Triton
- [ComputeEval](#202504-computeeval-evaluating-large-language-models-for-cuda-code-generation): Text2CUDA
- [HPCTransEval & KernelBench_C](#202506-hpctranscompile-an-ai-compiler-generated-dataset-for-high-performance-cuda-transpilation-and-llm-preliminary-exploration): CUDA2C / C++
- [BabelTower](#202506-neurips-2025-qimeng-mupa-mutual-supervised-learning-for-sequential-to-parallel-code-translation): C++2CUDA
- [MultiKernelBench](#202507-multikernelbench-a-multi-platform-benchmark-for-kernel-generation): Torch2CUDA, Torch2AscendC (Huawei NPU), Torch2Pallas (Google TPU)
- [TritonGym](#202510-tritongym-a-benchmark-for-agentic-llm-workflows-in-triton-gpu-code-generation): Torch2Triton

### âš™ï¸ Fine-tuning for Kernel Generation

- [KernelLLM](#202504-making-kernel-development-more-accessible-with-kernelllm): SFT
- [HPCTransCompile](#202506-hpctranscompile-an-ai-compiler-generated-dataset-for-high-performance-cuda-transpilation-and-llm-preliminary-exploration): SFT
- [QiMeng-MuPa](#202506-neurips-2025-qimeng-mupa-mutual-supervised-learning-for-sequential-to-parallel-code-translation): SFT
- [AutoTriton](#202507-autotriton-automatic-triton-programming-with-reinforcement-learning-in-llms): SFT + RL (GRPO)
- [Kevin](#202507-kevin-multi-turn-rl-for-generating-cuda-kernels): Multi-turn RL (GRPO)
- [CUDA-L1](#202507-cuda-l1-improving-cuda-optimization-via-contrastive-reinforcement-learning): Contrastive RL
- [CudaLLM](#202508-cudallm-training-language-models-to-generate-high-performance-cuda-kernels): SFT + RL
- [ConCuR](#202510-concur-conciseness-makes-state-of-the-art-kernel-generation): SFT
- [TritonRL](#202510-tritonrl-training-llms-to-think-and-code-triton-without-cheating): SFT + RL (GRPO)
- [Performance Tool](#202510-integrating-performance-tools-in-model-reasoning-for-gpu-kernel-optimization): RL (GRPO)
- [SparseRL](#202510-mastering-sparse-cuda-generation-through-pretrained-models-and-deep-reinforcement-learning): Pretrain + SFT + RL
- [QiMeng-Kernel](#202511-aaai-2026-qimeng-kernel-macro-thinking-micro-coding-paradigm-for-llm-based-high-performance-gpu-kernel-generation): RL (RLVR)
- [CUDA-L2](#202512-cuda-l2-surpassing-cublas-performance-for-matrix-multiplication-through-reinforcement-learning): RL


### ğŸ” Self-Refinement & Iterative Optimization

- [CUDA-LLM](#202506-cuda-llm-llms-can-write-efficient-cuda-kernels): Single LLM
- [GPU Kernel Scientist](#202506-gpu-kernel-scientist-an-llm-driven-framework-for-iterative-kernel-optimization): Multiple LLMs (`Evolutionary Selector`, `Experimental Designer`, `Kernel Writer`)
- [GEAK](#202507-geak-introducing-triton-kernel-ai-agent--evaluation-benchmarks): Agents
- [SwizzlePerf](#202508-swizzleperf-hardware-aware-llms-for-gpu-kernel-performance-optimization): Single LLM to perform `swizzling`, FeedBack by L2 Hit Rate
- [Astra](#202509-astra-a-multi-agent-system-for-gpu-kernel-performance-optimization): Agents
- [RISC-V Kernels](#202509-evolution-of-kernels-automated-risc-v-kernel-optimization-with-large-language-models): Single LLM
- [RobustAgent](#202509-towards-robust-agentic-cuda-kernel-benchmarking-verification-and-optimization): Agents
- [EvoEngineer](#202510-evoengineer-mastering-automated-cuda-kernel-code-evolution-with-large-language-models): Single LLM
- [STARK](#202510-stark-strategic-team-of-agents-for-refining-kernels): Agents
- [CudaForge](#202511-cudaforge-an-agent-framework-with-hardware-feedback-for-cuda-kernel-optimization): Agents
- [PRAGMA](#202511-pragma-a-profiling-reasoned-multi-agent-framework-for-automatic-kernel-optimization): Agents
- [KernelFalcon](#202511-kernelfalcon-autonomous-gpu-kernel-generation-via-deep-agents): Agents
- [PIKE-B](#202511-optimizing-pytorch-inference-with-llm-based-multi-agent-systems): Agents
- [TritonForge](#202512-tritonforge-profiling-guided-framework-for-automated-triton-kernel-optimization): Multiple LLMs (`Test Generation`, `Kernel Optimization`)

### ğŸ§© Other Methods

- [ReGraphT](#202510-from-large-to-small-transferring-cuda-optimization-expertise-via-reasoning-graph): Reasoning Graph + Monte Carol Search
- [MaxCode](#202510-maxcode-a-max-reward-reinforcement-learning-framework-for-automated-code-optimization): Classical RL

---

## ğŸ“˜ All Papers (Sorted By Date)

### (2025.02) [ICML 2025] KernelBench: Can LLMs Write Efficient GPU Kernels?

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2502.10517)
>  
> ğŸ› ï¸ [Code](https://github.com/ScalingIntelligence/KernelBench) ![Stars](https://img.shields.io/github/stars/ScalingIntelligence/KernelBench.svg)

### (2025.02) TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2502.14752)
>  
> ğŸ› ï¸ [Code](https://github.com/thunlp/TritonBench) ![Stars](https://img.shields.io/github/stars/thunlp/TritonBench.svg)

### (2025.04) ComputeEval: Evaluating Large Language Models for CUDA Code Generation

> ğŸ› ï¸ [Code](https://github.com/NVIDIA/compute-eval) ![Stars](https://img.shields.io/github/stars/NVIDIA/compute-eval.svg)

### (2025.04) Making Kernel Development more accessible with KernelLLM

> ğŸ¤— [Model](https://huggingface.co/facebook/KernelLLM)

### (2025.06) CUDA-LLM: LLMs Can Write Efficient CUDA Kernels

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2506.09092)

### (2025.06) HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2506.10401)
>  
> ğŸ› ï¸ [Code](https://github.com/PJLAB-CHIP/HPCTransCompile) ![Stars](https://img.shields.io/github/stars/PJLAB-CHIP/HPCTransCompile.svg)

### (2025.06) [NeurIPS 2025] QiMeng-MuPa: Mutual-Supervised Learning for Sequential-to-Parallel Code Translation

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2506.11153)
>  
> ğŸ› ï¸ [Code](https://github.com/QiMeng-IPRC/QiMeng-MuPa) ![Stars](https://img.shields.io/github/stars/QiMeng-IPRC/QiMeng-MuPa.svg)

### (2025.06) GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2506.20807)

### (2025.07) AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2507.05687)
>  
> ğŸ› ï¸ [Code](https://github.com/AI9Stars/AutoTriton) ![Stars](https://img.shields.io/github/stars/AI9Stars/AutoTriton.svg)

### (2025.07) Kevin: Multi-Turn RL for Generating CUDA Kernels

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2507.11948)

### (2025.07) CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2507.14111)
>  
> ğŸ› ï¸ [Code](https://github.com/deepreinforce-ai/CUDA-L1) ![Stars](https://img.shields.io/github/stars/deepreinforce-ai/CUDA-L1.svg)

### (2025.07) MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2507.17773)
>  
> ğŸ› ï¸ [Code](https://github.com/wzzll123/MultiKernelBench) ![Stars](https://img.shields.io/github/stars/wzzll123/MultiKernelBench.svg)

### (2025.07) Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2507.23194)
>  
> ğŸ› ï¸ [Code](https://github.com/AMD-AGI/GEAK-agent) ![Stars](https://img.shields.io/github/stars/AMD-AGI/GEAK-agent.svg)

### (2025.08) CudaLLM: Training Language Models to Generate High-Performance CUDA Kernels

> ğŸ› ï¸ [Code](https://github.com/ByteDance-Seed/cudaLLM) ![Stars](https://img.shields.io/github/stars/ByteDance-Seed/cudaLLM.svg)

### (2025.08) SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2508.20258)

### (2025.09) Astra: A Multi-Agent System for GPU Kernel Performance Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2509.07506)
>  
> ğŸ› ï¸ [Code](https://github.com/Anjiang-Wei/Astra) ![Stars](https://img.shields.io/github/stars/Anjiang-Wei/Astra.svg)

### (2025.09) Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2509.14265)

### (2025.09) Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2509.14279)
>  
> ğŸ› ï¸ [Code](https://github.com/SakanaAI/robust-kbench) ![Stars](https://img.shields.io/github/stars/SakanaAI/robust-kbench.svg)

### (2025.10) EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2510.03760)

### (2025.10) ConCuR: Conciseness Makes State-of-the-Art Kernel Generation

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2510.07356)

### (2025.10) TritonRL: Training LLMs to Think and Code Triton Without Cheating

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2510.17891)

### (2025.10) STARK: Strategic Team of Agents for Refining Kernels

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2510.16996)

### (2025.10) Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2510.17158)

### (2025.10) From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2510.19873)

### (2025.10) Mastering Sparse CUDA Generation through Pretrained Models and Deep Reinforcement Learning

> ğŸ“ƒ [Paper](https://openreview.net/forum?id=VdLEaGPYWT)

### (2025.10) TritonGym: A Benchmark for Agentic LLM Workflows in Triton GPU Code Generation

> ğŸ“ƒ [Paper](https://openreview.net/forum?id=oaKd1fVgWc)

### (2025.10) MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization

> ğŸ“ƒ [Paper](https://openreview.net/pdf?id=E4RYoWcbl2)

### (2025.11) CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2511.01884)
>  
> ğŸ› ï¸ [Code](https://github.com/OptimAI-Lab/CudaForge) ![Stars](https://img.shields.io/github/stars/OptimAI-Lab/CudaForge.svg)

### (2025.11) PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2511.06345)

### (2025.11) KernelFalcon: Autonomous GPU Kernel Generation via Deep Agents

> ğŸ“ƒ [Blog](https://pytorch.org/blog/kernelfalcon-autonomous-gpu-kernel-generation-via-deep-agents/)
>  
> ğŸ› ï¸ [Code](https://github.com/meta-pytorch/KernelAgent) ![Stars](https://img.shields.io/github/stars/meta-pytorch/KernelAgent.svg)

### (2025.11) KForge: Program Synthesis for Diverse AI Hardware Accelerators

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2511.13274)

### (2025.11) AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2511.15915)


### (2025.11) Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2511.16964)

### (2025.11) KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2511.18868)

### (2025.11) [AAAI 2026] QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2511.20100)

### (2025.12) CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2512.02551)
>  
> ğŸ› ï¸ [Code](https://github.com/deepreinforce-ai/CUDA-L2) ![Stars](https://img.shields.io/github/stars/deepreinforce-ai/CUDA-L2.svg)

### (2025.12) TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization

> ğŸ“ƒ [Paper](https://arxiv.org/abs/2512.09196)

---

## âœ¨ Contributing

Welcome to star â­ and open an issue or PR to improve this repo!
